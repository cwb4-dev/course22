{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Building Neural Networks With PyTorch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Nc8sigOEA3Xf","colab_type":"text"},"source":["# PyTorch 101: Building Neural Networks \n","\n","In this tutorial, we are going to build a Resnet based Neural network to classify the CIFAR 10 Dataset. Before, we begin, let me say that the purpose of this tutorial is not to achieve the best possible accuracy on the task, but to show you how to use PyTorch."]},{"cell_type":"code","metadata":{"id":"3NmJoq9YA3Xi","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.utils.data\n","import torch.optim as optim\n","import numpy as np\n","import pickle\n","import os\n","from PIL import Image\n","import random\n","import time\n","import torchvision\n","\n","cuda_available = torch.cuda.is_available()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aXBsHO_IA3X5","colab_type":"text"},"source":["### Building the Network\n","While PyTorch provided many layers out of the box with it's torch.nn module, we will have to implement the residual block ourselves. We first begin by defining the the resnet block.  \n","\n","\n","## A Simple Neural Network\n","In this tutorial, we will be implementing a very simple neural network.   \n","\n","### Building the Network\n","The `torch.nn.Module` is the cornerstone of designing neural networks in PyTorch. This class can be used to implement a layer like a fully connected layer, a convolutional layer, a pooling layer, an activation function, and also an entire neural network by instantiating a `torch.nn.Module` object. (From now on, I'll refer to it as merely nn.module) \n","\n","Multiple `nn.Module` objects can be strung together to form a bigger nn.Module object, which is how we can implement a neural network using many layers. In fact, nn.Module can be used to represent an arbitrary function f in PyTorch.\n","\n","The nn.Module class has two methods that you have to override.\n","1. `__init__` function. This function is invoked when you create an instance of the nn.Module. Here you will define the various parameters of a layer such as filters, kernel size for a convolutional layer, dropout probability for the dropout layer. \n","\n","2. `forward` function. This is where you define how your output is computed. This function doesn't need to be explicitly called, and can be run by just calling the nn.Module instance like a function with the input as it's argument. "]},{"cell_type":"code","metadata":{"id":"Up7wAgO6OGJC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d2942f23-a46b-4272-b70c-afa4f434a1c1","executionInfo":{"status":"ok","timestamp":1557348877214,"user_tz":-330,"elapsed":1081,"user":{"displayName":"Ayoosh Kathuria","photoUrl":"https://lh5.googleusercontent.com/-hC2hkjwNr9s/AAAAAAAAAAI/AAAAAAAACpo/DPqp1uUqR4E/s64/photo.jpg","userId":"11533138969683019189"}}},"source":["class MyLayer(nn.Module):\n","  def __init__(self, param):\n","    super().__init__()\n","    self.param = param \n","  \n","  def forward(self, x):\n","    return x * self.param\n","  \n","myLayerObject = MyLayer(5)\n","output = myLayerObject(torch.Tensor([5, 4, 3]) )    #calling forward inexplicitly \n","print(output)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["tensor([25., 20., 15.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1AKR9RyZOnBi","colab_type":"text"},"source":["Another widely used and important class is the `nn.Sequential` class.  When initiating this class we can pass a list of `nn.Module` objects in a particular sequence. The object returned by `nn.Sequential` is itself a `nn.Module` object. When this object is run with an input, it sequentially runs the input through all the nn.Module object we passed to it, in the very same order as we passed them. "]},{"cell_type":"code","metadata":{"id":"lbYYWtMLOu29","colab_type":"code","colab":{}},"source":["combinedNetwork = nn.Sequential(MyLayer(5), MyLayer(10))\n","\n","output = combinedNetwork([3,4])\n","\n","#equivalent to..\n","# out = MyLayer(5)([3,4])\n","# out = MyLayer(10)(out)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SGoPAtVeO09a","colab_type":"text"},"source":["Let us now start implementing our classification network. We will make use of convolutional and pooling layers, as well as a custom implemented residual block.\n","\n","While PyTorch provided many layers out of the box with it's `torch.nn` module, we will have to implement the residual block ourselves. Before implementing the neural network, we implement the ResNet Block."]},{"cell_type":"code","metadata":{"id":"XE0FkyqQA3X7","colab_type":"code","colab":{}},"source":["class ResidualBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super(ResidualBlock, self).__init__()\n","        \n","        # Conv Layer 1\n","        self.conv1 = nn.Conv2d(\n","            in_channels=in_channels, out_channels=out_channels,\n","            kernel_size=(3, 3), stride=stride, padding=1, bias=False\n","        )\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        \n","        # Conv Layer 2\n","        self.conv2 = nn.Conv2d(\n","            in_channels=out_channels, out_channels=out_channels,\n","            kernel_size=(3, 3), stride=1, padding=1, bias=False\n","        )\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","    \n","        # Shortcut connection to downsample residual\n","        # In case the output dimensions of the residual block is not the same \n","        # as it's input, have a convolutional layer downsample the layer \n","        # being bought forward by approporate striding and filters\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(\n","                    in_channels=in_channels, out_channels=out_channels,\n","                    kernel_size=(1, 1), stride=stride, bias=False\n","                ),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","    def forward(self, x):\n","        out = nn.ReLU()(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = nn.ReLU()(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bWGczWE5A3YB","colab_type":"text"},"source":["Now, we can define our full network. "]},{"cell_type":"code","metadata":{"id":"XEyc3RRnA3YE","colab_type":"code","colab":{}},"source":["class ResNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(ResNet, self).__init__()\n","        \n","        # Initial input conv\n","        self.conv1 = nn.Conv2d(\n","            in_channels=3, out_channels=64, kernel_size=(3, 3),\n","            stride=1, padding=1, bias=False\n","        )\n","        self.bn1 = nn.BatchNorm2d(64)\n","        \n","        # Create blocks\n","        self.block1 = self._create_block(64, 64, stride=1)\n","        self.block2 = self._create_block(64, 128, stride=2)\n","        self.block3 = self._create_block(128, 256, stride=2)\n","        self.block4 = self._create_block(256, 512, stride=2)\n","        self.linear = nn.Linear(512, num_classes)\n","    \n","    # A block is just two residual blocks for ResNet18\n","    def _create_block(self, in_channels, out_channels, stride):\n","        return nn.Sequential(\n","            ResidualBlock(in_channels, out_channels, stride),\n","            ResidualBlock(out_channels, out_channels, 1)\n","        )\n","\n","    def forward(self, x):\n","        out = nn.ReLU()(self.bn1(self.conv1(x)))\n","        out = self.block1(out)\n","        out = self.block2(out)\n","        out = self.block3(out)\n","        out = self.block4(out)\n","        out = nn.AvgPool2d(4)(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMQ5tCBAA3YQ","colab_type":"text"},"source":["## Loading Data\n","We first start by downloading the CIFAR-10 dataset in the same directory as our code file. "]},{"cell_type":"code","metadata":{"id":"NttPirgQA3YR","colab_type":"code","outputId":"d6a7b4bb-aa9d-4a76-8780-b93fadff559d","executionInfo":{"status":"ok","timestamp":1556716696613,"user_tz":-330,"elapsed":21390,"user":{"displayName":"Ayoosh Kathuria","photoUrl":"https://lh5.googleusercontent.com/-hC2hkjwNr9s/AAAAAAAAAAI/AAAAAAAACpo/DPqp1uUqR4E/s64/photo.jpg","userId":"11533138969683019189"}},"colab":{"base_uri":"https://localhost:8080/","height":301}},"source":["!wget http://pjreddie.com/media/files/cifar.tgz\n","!tar xzf cifar.tgz"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-05-01 13:18:06--  http://pjreddie.com/media/files/cifar.tgz\n","Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://pjreddie.com/media/files/cifar.tgz [following]\n","--2019-05-01 13:18:07--  https://pjreddie.com/media/files/cifar.tgz\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 168584360 (161M) [application/octet-stream]\n","Saving to: ‘cifar.tgz’\n","\n","\rcifar.tgz             0%[                    ]       0  --.-KB/s               \rcifar.tgz             0%[                    ]   1.46M  7.30MB/s               \rcifar.tgz             8%[>                   ]  13.01M  32.5MB/s               \rcifar.tgz            16%[==>                 ]  27.30M  45.5MB/s               \rcifar.tgz            26%[====>               ]  41.91M  52.3MB/s               \rcifar.tgz            35%[======>             ]  56.68M  56.4MB/s               \rcifar.tgz            44%[=======>            ]  71.41M  59.2MB/s               \rcifar.tgz            53%[=========>          ]  85.87M  61.0MB/s               \rcifar.tgz            62%[===========>        ]  99.71M  62.0MB/s               \rcifar.tgz            70%[=============>      ] 113.96M  63.0MB/s               \rcifar.tgz            79%[==============>     ] 128.26M  63.9MB/s               \rcifar.tgz            88%[================>   ] 142.68M  64.6MB/s               \rcifar.tgz            97%[==================> ] 157.12M  65.2MB/s               \rcifar.tgz           100%[===================>] 160.77M  65.5MB/s    in 2.5s    \n","\n","2019-05-01 13:18:09 (65.5 MB/s) - ‘cifar.tgz’ saved [168584360/168584360]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ATOKGqZeA3Yk","colab_type":"text"},"source":["We now read the labels of the classes present in the CIFAR dataset."]},{"cell_type":"code","metadata":{"id":"WTYvB3-sA3Yn","colab_type":"code","colab":{}},"source":["data_dir = \"cifar/train/\"\n","\n","with open(\"cifar/labels.txt\") as label_file:\n","    labels = label_file.read().split()\n","    label_mapping = dict(zip(labels, list(range(len(labels)))))\n","     \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_321dnrA3Yu","colab_type":"text"},"source":["We then write a preprocessing function that will take in a `PIL.Image` that will \n","\n","1. Randomly horizontally the image with a probability of 0.5 \n","2. Normalise the image with mean and standard deviation of CIFAR dataset\n","3. Reshape it from W X H X C to C X H X W. \n"]},{"cell_type":"code","metadata":{"id":"2s3SzXI5A3Y1","colab_type":"code","colab":{}},"source":["def preprocess(image):\n","  \n","    image = np.array(image)\n","    \n","    \n","    if random.random() > 0.5:\n","        image = image[::-1,:,:]\n","    \n","    cifar_mean = np.array([0.4914, 0.4822, 0.4465]).reshape(1,1,-1)\n","    cifar_std  = np.array([0.2023, 0.1994, 0.2010]).reshape(1,1,-1)\n","    \n","    \n","    image = (image - cifar_mean) / cifar_std\n","      \n","    image = image.transpose(2,1,0)\n","    return image\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCJB1qDWA3Y9","colab_type":"text"},"source":["\n","### Input Format\n","\n","The input format for images is `[B C H W]`. Where `B` is the batch size, `C` are the channels, `H` is the height and `W` is the width. \n","\n","Normally, there are two classes PyTorch provides you in relation to build input pipelines to load data. \n","\n","1. `torch.data.utils.dataset`, which we will just refer as the `dataset` class now. \n","2. `torch.data.utils.dataloader` , which we will just refer as the `dataloader` class now. \n","\n","\n","### torch.utils.data.dataset\n","\n","`dataset` is a class that loads the data and returns a generator so that you iterate over it. It also lets you incorporate data augmentation techniques into the input Pipeline. \n","\n","If you want to create a `dataset` object for your data, you need to overload three functions. \n","\n","1. `__init__` function. Here, you define things related to your dataset here. Most importantly, the location of your data. You can also define various data augmentations you want to apply.\n","2. `__len__` function. Here, you just return the length of the dataset. \n","3. `__getitem__` function. The function takes as an argument an index `i` and returns a data example. This function would be called every iteration during our training loop with a different i by the dataset object. \n","\n","Here is a implementation of our dataset object for the CIFAR dataset. \n"]},{"cell_type":"code","metadata":{"id":"gDY2Ig9UA3ZD","colab_type":"code","colab":{}},"source":["class Cifar10Dataset(torch.utils.data.Dataset):\n","    def __init__(self, data_dir, data_size = 0, transforms = None):\n","        files = os.listdir(data_dir)\n","        files = [os.path.join(data_dir,x) for x in files]\n","        \n","        \n","        if data_size < 0 or data_size > len(files):\n","            assert(\"Data size should be between 0 to number of files in the dataset\")\n","        \n","        if data_size == 0:\n","            data_size = len(files)\n","        \n","        self.data_size = data_size\n","        self.files = random.sample(files, self.data_size)\n","        self.transforms = transforms\n","        \n","    def __len__(self):\n","        return self.data_size\n","    \n","    def __getitem__(self, idx):\n","        image_address = self.files[idx]\n","        image = Image.open(image_address)\n","        image = preprocess(image)\n","        label_name = image_address[:-4].split(\"_\")[-1]\n","        label = label_mapping[label_name]\n","        \n","        image = image.astype(np.float32)\n","        \n","        if self.transforms:\n","            image = self.transforms(image)\n","\n","        return image, label\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m18xYCmhA3ZX","colab_type":"text"},"source":["### torch.utils.data.Dataloader\n","The `Dataloader` class facilitates \n","1. Batching of Data\n","2. Shuffling of Data \n","3. Loading multiple data at a single time using threads \n","4. Prefetching, that is, while GPU crunches the current batch, Dataloader can load the next batch into memory in meantime. This means GPU doesn't have to wait for the next batch and it speeds up training.\n","\n","You instantiate a `Dataloader` object with a `Dataset` object. Then you can iterate over a Dataloader object instance just like you do with any python generator \n"]},{"cell_type":"code","metadata":{"id":"0ACgPBSsA3ZZ","colab_type":"code","colab":{}},"source":["trainset = Cifar10Dataset(data_dir = \"cifar/train/\", transforms=None)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","\n","testset = Cifar10Dataset(data_dir = \"cifar/test/\", transforms=None)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=True, num_workers=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kC3QKyolA3Zm","colab_type":"text"},"source":["\n","## Training and Evaluation\n","\n","We now define an optimiser for our training. We use a cross entropy loss, with momentum based SGD optimisation algorithm. Our learning rate is decayed by a factor of 0.1 at 150th and 200th epoch. \n","\n","For this we use the `torch.optim` class, which provides us with the `SGD` function that implements mini batch stochastic gradient descent with momentum. We pass `net.parameters()` to the function `SGD`. `net.parameters()` is actually a list of all the trainable parameters of our network, and by passing them to `SGD` we make sure that at each step, `SGD` updates them. \n","\n","We also use `optim.lr_scheduler.MultiStepLR` class. We pass out optimiser object to this class. By calling `step` on an object of this class, we make sure that the parameters like learning rate are updated for `optim` accordingly. "]},{"cell_type":"code","metadata":{"id":"eUY_J5XWA3Zo","colab_type":"code","colab":{}},"source":["clf = ResNet()\n","if cuda_available:\n","    clf = clf.cuda()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(clf.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 200], gamma=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"It2XagTeA3Zv","colab_type":"text"},"source":["We finally train for 20 epochs. You can increase the number of epochs. This might take a while on a GPU. Again the idea of this tutorial is to show how PyTorch works and not to attain the best accuracy.\n","\n","\n","We evaluate classification accuracy every epoch."]},{"cell_type":"code","metadata":{"id":"EsUUPP_xA3Zx","colab_type":"code","outputId":"a7623b2a-dbad-4bff-991b-0a628d342ef4","executionInfo":{"status":"error","timestamp":1556717391713,"user_tz":-330,"elapsed":593069,"user":{"displayName":"Ayoosh Kathuria","photoUrl":"https://lh5.googleusercontent.com/-hC2hkjwNr9s/AAAAAAAAAAI/AAAAAAAACpo/DPqp1uUqR4E/s64/photo.jpg","userId":"11533138969683019189"}},"colab":{"base_uri":"https://localhost:8080/","height":1089}},"source":["for epoch in range(10):\n","    losses = []\n","    scheduler.step()\n","    # Train\n","    start = time.time()\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        if cuda_available:\n","            inputs, targets = inputs.cuda(), targets.cuda()            # If the inputs and network is on different GPUs, PyTorch will give an error\n","\n","           \n","        optimizer.zero_grad()                                          # If we don't do this, then the new grads will be merely added to the value \n","                                                                       # of gradient computed in the previous step, i.e gradients will be accumulated \n","                                                                       # over iterations. To prevent this, we set the grad to zero.\n","            \n","        outputs = clf(inputs)                                          # Compute output\n","        \n","        loss = criterion(outputs, targets)                             # Compute Loss   \n","        \n","        loss.backward()                                                # Compute Gradients \n","        \n","        optimizer.step()                                               # Update the values of net.params() with the computed gradients\n","        \n","        losses.append(loss.item())        \n","      \n","    # Evaluate\n","    clf.eval()                                                         # A network has a eval mode and a train mode. This is related to how layers like \n","                                                                       # Batch Norm and Dropout have different behaviours during inference (eval()) and \n","                                                                       # training (train())\n","        \n","    total = 0\n","    correct = 0\n","    \n","    with torch.no_grad():                                             # Put the inference code in the `torch.no_grad()` context manager so that no graph \n","                                                                      # is created and memory is saved (We don't need graphs as we don't backprop)\n","        \n","      for batch_idx, (inputs, targets) in enumerate(testloader):\n","          if cuda_available:\n","              inputs, targets = inputs.cuda(), targets.cuda()\n","\n","          outputs = clf(inputs)\n","          _, predicted = torch.max(outputs.data, 1)\n","          total += targets.size(0)\n","          correct += predicted.eq(targets.data).cpu().sum()\n","\n","      print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n","      print('--------------------------------------------------------------')\n","    clf.train()    \n","\n","writer.close()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch : 0 Loss : 2.310 Time : 0.477 seconds \n","Epoch : 100 Loss : 1.962 Time : 19.268 seconds \n","Epoch : 200 Loss : 1.830 Time : 20.404 seconds \n","Epoch : 300 Loss : 1.735 Time : 21.622 seconds \n","Epoch : 0 Test Acc : 46.000\n","--------------------------------------------------------------\n","Epoch : 0 Loss : 1.269 Time : 0.355 seconds \n","Epoch : 100 Loss : 1.335 Time : 20.406 seconds \n","Epoch : 200 Loss : 1.282 Time : 20.475 seconds \n","Epoch : 300 Loss : 1.236 Time : 20.828 seconds \n","Epoch : 1 Test Acc : 55.000\n","--------------------------------------------------------------\n","Epoch : 0 Loss : 0.983 Time : 0.354 seconds \n","Epoch : 100 Loss : 1.011 Time : 20.605 seconds \n","Epoch : 200 Loss : 0.999 Time : 20.538 seconds \n","Epoch : 300 Loss : 0.979 Time : 20.531 seconds \n","Epoch : 2 Test Acc : 59.000\n","--------------------------------------------------------------\n","Epoch : 0 Loss : 0.788 Time : 0.364 seconds \n","Epoch : 100 Loss : 0.852 Time : 20.558 seconds \n","Epoch : 200 Loss : 0.852 Time : 20.571 seconds \n","Epoch : 300 Loss : 0.842 Time : 20.535 seconds \n","Epoch : 3 Test Acc : 66.000\n","--------------------------------------------------------------\n","Epoch : 0 Loss : 0.937 Time : 0.350 seconds \n","Epoch : 100 Loss : 0.786 Time : 20.576 seconds \n","Epoch : 200 Loss : 0.776 Time : 20.540 seconds \n","Epoch : 300 Loss : 0.767 Time : 20.465 seconds \n","Epoch : 4 Test Acc : 61.000\n","--------------------------------------------------------------\n","Epoch : 0 Loss : 0.755 Time : 0.346 seconds \n","Epoch : 100 Loss : 0.694 Time : 20.482 seconds \n","Epoch : 200 Loss : 0.699 Time : 20.434 seconds \n","Epoch : 300 Loss : 0.699 Time : 20.438 seconds \n","Epoch : 5 Test Acc : 69.000\n","--------------------------------------------------------------\n","Epoch : 0 Loss : 0.631 Time : 0.361 seconds \n","Epoch : 100 Loss : 0.635 Time : 20.456 seconds \n","Epoch : 200 Loss : 0.647 Time : 20.431 seconds \n","Epoch : 300 Loss : 0.650 Time : 20.428 seconds \n","Epoch : 6 Test Acc : 72.000\n","--------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-7fb8e6be4d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;31m# need to call `.task_done()` because we don't use `.join()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}
